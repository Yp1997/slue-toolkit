
<p style="font: 16px Monaco; margin-left:0em; color:#eb6e6e;">
<b>Interspeech 2022 Special Session</b></p>

# "Low-Resource Spoken Language Understanding"

Progress in speech processing has been facilitated by shared datasets and benchmarks. Historically these have focused on automatic speech recognition (ASR), speaker identification, or other lower-level tasks. Interest has been growing in higher-level spoken language understanding (SLU) tasks, including using end-to-end models, but there are fewer annotated datasets for such tasks, and the existing datasets tend to be relatively small. At the same time, recent work shows the possibility of pre-training generic representations and then fine-tuning for several tasks using relatively little labeled data.

In this special session, we would like to foster a discussion and invite researchers in the field of SLU working on tasks such as named entity recognition (NER), sentiment analysis, intent classification, dialogue act tagging, or others, using either audio or ASR transcripts.

We invite contributions for any relevant work in low-resource SLU problems, which include (but are not limited to):

- Training/fine-tuning approach using self/semi-supervised model for SLU tasks
- Comparison between pipeline and end-to-end SLU systems
- Self/semi-supervised learning approach focusing on SLU
- Multi-task/transfer/student-teacher learning focusing on SLU tasks
- Theoretical or empirical study on low-resource SLU problems


<p style="font: 16px Monaco; margin-left:0em; color:#eb6e6e;">
<b>Resources
</b></p>

For this special session, we will provide support for several benchmark tasks using the new Spoken Language Understanding Evaluation (SLUE) benchmark suite (https://arxiv.org/abs/2111.10367). SLUE includes annotation for ASR, NER and sentiment analysis. We also provide a toolkit to pre-process and fine-tune scripts for baseline models. It is not mandatory for submissions to use SLUE, but we offer it as a well-defined experiment setting for low-resource SLU.

- SLUE Dataset: \
    - [slue-voxceleb](https://papers-slue.awsdev.asapp.com/slue-voxceleb_blind.tar.gz)
    - [slue-voxpopuli](https://papers-slue.awsdev.asapp.com/slue-voxpopuli_blind.tar.gz)
- SLUE Toolkit: [Github repo](https://github.com/asappresearch/slue-toolkit)
- SLUE Website: [https://asappresearch.github.io/slue-toolkit](https://asappresearch.github.io/slue-toolkit)

The other datasets/benchmarks we recommend are (alphabetical order)

- [ASR-GLUE](https://arxiv.org/abs/2108.13048)
- [ESPnet-SLU](https://arxiv.org/pdf/2111.14706.pdf)
- [SLURP](https://arxiv.org/abs/2011.13205)
- [SUPERB](http://superbbenchmark.org)
- [Timers and Such](https://arxiv.org/abs/2104.01604)

<p style="font: 16px Monaco; margin-left:0em; color:#eb6e6e;">
<b>Paper submission
</b></p>

Papers for Interspeech Special Session have to be submitted following the same schedule and procedure as regular papers of INTERSPEECH 2022. The submitted papers will undergo the same review process by anonymous and independent reviewers.

Submission URL : (TBA)

<p style="font: 16px Monaco; margin-left:0em; color:#eb6e6e;">
<b>Important dates
</b></p>

<p style="font: 12px Monaco; margin-left:0em; color:black;">
Paper submission due  &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&nbsp;&nbsp;&nbsp; : Mar. 21, 2022<br>
Paper update due &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&nbsp;&nbsp;&nbsp;&nbsp;: Mar. 28, 2022<br>
Acceptance notification date &emsp; : Jun. 13, 2022<br>
Final paper upload &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;: Jun. 23, 2022<br>
Conference date &emsp;&emsp;&emsp;&emsp;&emsp;&emsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;: Sep. 18-22, 2022
</p>

<p style="font: 16px Monaco; margin-left:0em;color:#eb6e6e;">
<b>Organizers
</b></p>

Suwon Shon - ASAPP
<br>Felix Wu - ASAPP
<br>Pablo Brusco - ASAPP
<br>Kyu J. Han - ASAPP
<br>Karen Livescu - TTI at Chicago
<br>Ankita Pasad - TTI at Chicago
<br>Yoav Artzi - Cornell University
<br>Katrin Kirchhoff- Amazon
<br>Samuel R. Bowman - New York University
<br>Zhou Yu - Columbia University
